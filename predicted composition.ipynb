{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16676345-e652-4e55-848f-772b68012ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0.0, 'reg_lambda': 5, 'subsample': 0.8}\n",
      "Train MSE: 6742.067585708921\n",
      "Train RÂ²: 0.9324359944521288\n",
      "Test MSE: 41740.43965775887\n",
      "Test RÂ²: 0.6017122671762289\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# ================== Load and Preprocess Data ==================\n",
    "df = pd.read_excel(r\"C:\\Users\\Tanvi Parihar\\OneDrive\\Documents\\hydrogen embrittlement dataset .xlsx\")\n",
    "# Drop columns with all missing values\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# Fill any remaining NaNs\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=['ys'])\n",
    "y = df['ys']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# ================== XGBoost with Hyperparameter Tuning ==================\n",
    "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200,400],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.03, 0.05],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1],   # L1 regularization (lasso)\n",
    "    'reg_lambda': [1,5]    # L2 regularization (ridge)\n",
    "}\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# ================== Predictions ==================\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# ================== Evaluation ==================\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Train MSE:\", mean_squared_error(y_train, y_train_pred))\n",
    "print(\"Train RÂ²:\", r2_score(y_train, y_train_pred))\n",
    "print(\"Test MSE:\", mean_squared_error(y_test, y_test_pred))\n",
    "print(\"Test RÂ²:\", r2_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6efdc037-650c-4a75-8793-d102eb7aa165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0.0, 'reg_lambda': 5, 'subsample': 0.8}\n",
      "Train MSE: 6742.067585708921\n",
      "Train RÂ²: 0.9324359944521288\n",
      "Test MSE: 41740.43965775887\n",
      "Test RÂ²: 0.6017122671762289\n",
      "Starting reverse design...\n",
      "Target ys: 850 Â± 25 MPa\n",
      "Training data ys range: 0.00 - 1550.00 MPa\n",
      "Generating 50000 random samples...\n",
      "Predicted ys range: 216.00 - 1143.75 MPa\n",
      "\n",
      "âœ… Found 1635 candidate compositions for target ys â‰ˆ 850 MPa\n",
      "\n",
      "          Fe        Ni         C        Mn        Cr        Mo        Si  \\\n",
      "0  96.499622  1.305318  0.335696  0.404061  6.296015  0.550111  0.458155   \n",
      "1  92.259716  4.557561  0.261543  0.000000  3.007011  0.218132  0.280243   \n",
      "2  92.024102  4.908673  0.304609  0.000000  1.484753  0.321953  0.081117   \n",
      "3  97.882551  2.266023  0.268168  0.472544  0.000000  0.178480  0.150878   \n",
      "4  94.354308  2.162377  0.472538  0.000000  9.919571  0.341934  0.187689   \n",
      "5  92.195093  2.206426  0.158615  1.089791  0.940195  0.334658  0.423387   \n",
      "6  93.958561  3.897094  0.140558  0.164160  0.404294  0.034423  0.125345   \n",
      "7  90.627222  2.432846  0.396311  2.797621  0.981928  0.534230  0.476348   \n",
      "8  92.111255  1.192195  0.492192  4.470355  1.078985  0.000000  0.306053   \n",
      "9  93.573405  3.538551  0.227192  2.642556  0.024549  0.198903  0.000000   \n",
      "\n",
      "   atomic concentration  electron affinity  ionic radii  work function  \\\n",
      "0          1.961031e+23          82.914173     0.780481       4.467334   \n",
      "1          3.432974e+23          72.890007     0.773097       4.505800   \n",
      "2          4.064186e+23          82.616020     0.782468       4.515791   \n",
      "3          2.789477e+23          82.956924     0.779482       4.520295   \n",
      "4          1.148775e+23          76.534429     0.782977       4.514512   \n",
      "5          2.223692e+23          73.538081     0.766735       4.486526   \n",
      "6          1.310444e+23          72.055612     0.768633       4.462824   \n",
      "7          3.000158e+23          80.645572     0.776760       4.521799   \n",
      "8          1.914122e+23          81.983476     0.787280       4.518880   \n",
      "9          1.780090e+23          76.578738     0.768900       4.515212   \n",
      "\n",
      "   first ionization  valence electron  ionic radii.1  enthalpy of vacancies  \\\n",
      "0       1540.498073          4.477719       8.697443             171.887564   \n",
      "1       1621.223471          4.505592       2.379384             163.192336   \n",
      "2       1770.243089          4.463429       0.689856             153.943976   \n",
      "3       1691.388205          4.488568       0.689856             170.277806   \n",
      "4       1287.701618          4.514265       0.689856             172.183640   \n",
      "5       1577.420306          4.467900      13.867191             165.669826   \n",
      "6       1514.463835          4.489619       0.689856             172.085153   \n",
      "7       1287.265000          4.502614       0.689856             162.713363   \n",
      "8       1463.206563          4.502908       8.223009             190.468547   \n",
      "9       1501.405922          4.514041      19.203262             163.128370   \n",
      "\n",
      "   predicted_ys  \n",
      "0    825.003723  \n",
      "1    825.013916  \n",
      "2    825.035217  \n",
      "3    825.039612  \n",
      "4    825.062195  \n",
      "5    825.066345  \n",
      "6    825.079590  \n",
      "7    825.087463  \n",
      "8    825.090637  \n",
      "9    825.118408  \n",
      "\n",
      "ðŸ† BEST COMPOSITION (closest to target 850 MPa):\n",
      "============================================================\n",
      "Predicted ys: 850.04 MPa\n",
      "Deviation from target: 0.04 MPa\n",
      "\n",
      "Optimal feature values:\n",
      "  Fe: 90.149310\n",
      "  Ni: 0.000000\n",
      "  C: 0.621827\n",
      "  Mn: 0.000000\n",
      "  Cr: 6.122864\n",
      "  Mo: 0.340088\n",
      "  Si: 0.295313\n",
      "  atomic concentration: 121635444355540296138752.000000\n",
      "  electron affinity: 72.557879\n",
      "  ionic radii: 0.772881\n",
      "  work function: 4.507103\n",
      "  first ionization: 1392.282350\n",
      "  valence electron: 4.514973\n",
      "  ionic radii.1: 3.895166\n",
      "  enthalpy of vacancies: 171.130991\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# ================== Load and Preprocess Data ==================\n",
    "df = pd.read_excel(r\"C:\\Users\\Tanvi Parihar\\OneDrive\\Documents\\hydrogen embrittlement dataset .xlsx\")\n",
    "# Drop columns with all missing values\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# Fill any remaining NaNs\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=['ys'])\n",
    "y = df['ys']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# ================== XGBoost with Hyperparameter Tuning ==================\n",
    "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200,400],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.03, 0.05],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1],   # L1 regularization (lasso)\n",
    "    'reg_lambda': [1,5]    # L2 regularization (ridge)\n",
    "}\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# ================== Predictions ==================\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# ================== Evaluation ==================\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Train MSE:\", mean_squared_error(y_train, y_train_pred))\n",
    "print(\"Train RÂ²:\", r2_score(y_train, y_train_pred))\n",
    "print(\"Test MSE:\", mean_squared_error(y_test, y_test_pred))\n",
    "print(\"Test RÂ²:\", r2_score(y_test, y_test_pred))\n",
    "\n",
    "# ================== Reverse Design ==================\n",
    "print(\"Starting reverse design...\")\n",
    "\n",
    "# Target properties\n",
    "target_ys = 850  # MPa\n",
    "tolerance = 25   # Â± range\n",
    "\n",
    "print(f\"Target ys: {target_ys} Â± {tolerance} MPa\")\n",
    "print(f\"Training data ys range: {y_train.min():.2f} - {y_train.max():.2f} MPa\")\n",
    "\n",
    "# Generate random samples\n",
    "n_samples = 50000  # Increased for better coverage\n",
    "print(f\"Generating {n_samples} random samples...\")\n",
    "\n",
    "# Expected features based on trained model\n",
    "expected_features = best_model.feature_names_in_\n",
    "samples = pd.DataFrame()\n",
    "\n",
    "# Generate samples using normal distribution around feature means\n",
    "for feature in expected_features:\n",
    "    col_data = X[feature].dropna()\n",
    "    min_val, max_val = col_data.min(), col_data.max()\n",
    "    mean_val, std_val = col_data.mean(), col_data.std()\n",
    "    \n",
    "    if np.issubdtype(col_data.dtype, np.number):\n",
    "        # Use normal distribution for better sampling\n",
    "        samples[feature] = np.random.normal(mean_val, std_val/2, n_samples)\n",
    "        # Clip to data bounds\n",
    "        samples[feature] = np.clip(samples[feature], min_val, max_val)\n",
    "    else:\n",
    "        # For categorical features\n",
    "        samples[feature] = np.random.choice(col_data.unique(), n_samples)\n",
    "\n",
    "# Predict using best_model\n",
    "predicted_ys = best_model.predict(samples)\n",
    "print(f\"Predicted ys range: {predicted_ys.min():.2f} - {predicted_ys.max():.2f} MPa\")\n",
    "\n",
    "# Filter compositions matching the target\n",
    "mask = (predicted_ys >= target_ys - tolerance) & (predicted_ys <= target_ys + tolerance)\n",
    "matches = samples[mask].copy()\n",
    "\n",
    "if len(matches) > 0:\n",
    "    matches['predicted_ys'] = predicted_ys[mask]\n",
    "    matches = matches.sort_values('predicted_ys')\n",
    "    \n",
    "    print(f\"\\n Found {len(matches)} candidate compositions for target ys â‰ˆ {target_ys} MPa\")\n",
    "    print()\n",
    "    \n",
    "    # Display results in table format like your example\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    \n",
    "    # Show only a reasonable number of results (top 10) and reset index\n",
    "    display_matches = matches.head(10) if len(matches) > 10 else matches\n",
    "    display_matches = display_matches.reset_index(drop=True)  # Remove original index numbers\n",
    "    print(display_matches.round(6))\n",
    "    \n",
    "    # Find and highlight the BEST composition (closest to target)\n",
    "    best_idx = (matches['predicted_ys'] - target_ys).abs().idxmin()\n",
    "    best_composition = matches.loc[best_idx]\n",
    "    \n",
    "    print(f\"\\n BEST COMPOSITION (closest to target {target_ys} MPa):\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Predicted ys: {best_composition['predicted_ys']:.2f} MPa\")\n",
    "    print(f\"Deviation from target: {abs(best_composition['predicted_ys'] - target_ys):.2f} MPa\")\n",
    "    print(\"\\nOptimal feature values:\")\n",
    "    for feature in expected_features:\n",
    "        print(f\"  {feature}: {best_composition[feature]:.6f}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n No compositions found for target {target_ys} Â± {tolerance} MPa\")\n",
    "    print(\"Try:\")\n",
    "    print(f\"1. Increase tolerance (current: Â±{tolerance})\")\n",
    "    print(f\"2. Adjust target value (current: {target_ys})\")\n",
    "    print(f\"3. Increase samples (current: {n_samples})\")\n",
    "    \n",
    "    # Show closest match\n",
    "    closest_idx = np.argmin(np.abs(predicted_ys - target_ys))\n",
    "    print(f\"\\nClosest prediction: {predicted_ys[closest_idx]:.2f} MPa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffd35e5a-8b6d-4d0e-a220-1ec8306dd8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (ys): 13286.765104062646\n",
      "\n",
      "Run 1:\n",
      "Best Parameters:\n",
      "  Fe: 88.2230\n",
      "  Ni: 12.2102\n",
      "  C: 1.8896\n",
      "  Mn: 0.0000\n",
      "  Cr: 14.6287\n",
      "  Mo: 3.5862\n",
      "  Si: 1.4699\n",
      "  atomic concentration: 265604703058984150499328.0000\n",
      "  electron affinity: 140.0000\n",
      "  ionic radii: 0.8100\n",
      "  work function: 4.6123\n",
      "  first ionization: 4114.2606\n",
      "  valence electron: 4.5300\n",
      "  enthalpy of vacancies: 180.0000\n",
      "Predicted ys: 1128.60\n",
      "\n",
      "Run 2:\n",
      "Best Parameters:\n",
      "  Fe: 84.4675\n",
      "  Ni: 8.1751\n",
      "  C: 3.0000\n",
      "  Mn: 0.0602\n",
      "  Cr: 10.6256\n",
      "  Mo: 1.6324\n",
      "  Si: 1.0120\n",
      "  atomic concentration: 299367706326616380538880.0000\n",
      "  electron affinity: 140.0000\n",
      "  ionic radii: 0.8097\n",
      "  work function: 5.0000\n",
      "  first ionization: 4859.4761\n",
      "  valence electron: 4.8866\n",
      "  enthalpy of vacancies: 180.0000\n",
      "Predicted ys: 1122.05\n",
      "\n",
      "Run 3:\n",
      "Best Parameters:\n",
      "  Fe: 96.5278\n",
      "  Ni: 17.2029\n",
      "  C: 1.8815\n",
      "  Mn: 2.6884\n",
      "  Cr: 1.6244\n",
      "  Mo: 2.2796\n",
      "  Si: 1.1418\n",
      "  atomic concentration: 319706509722261265580032.0000\n",
      "  electron affinity: 132.4263\n",
      "  ionic radii: 0.8099\n",
      "  work function: 4.3902\n",
      "  first ionization: 5121.6288\n",
      "  valence electron: 4.8914\n",
      "  enthalpy of vacancies: 177.2156\n",
      "Predicted ys: 1095.96\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from skopt import Optimizer\n",
    "from skopt.space import Real\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ========== Load Dataset ==========\n",
    "df = pd.read_excel(r\"C:\\Users\\Tanvi Parihar\\OneDrive\\Documents\\hydrogen embrittlement dataset .xlsx\")\n",
    "\n",
    "# Replace inf/-inf and drop NaNs\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# ========== Features and Target ==========\n",
    "opt_params = ['Fe', 'Ni', 'C', 'Mn', 'Cr', 'Mo', 'Si',\n",
    "              'atomic concentration', 'electron affinity', 'ionic radii',\n",
    "              'work function', 'first ionization', 'valence electron', 'enthalpy of vacancies']\n",
    "\n",
    "target = 'ys'  # Single target now\n",
    "\n",
    "X = df[opt_params]\n",
    "y = df[target]\n",
    "\n",
    "# ========== Train/Test Split ==========\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ========== Standardization ==========\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ========== Train Model ==========\n",
    "model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ========== Evaluate ==========\n",
    "print(\"MSE (ys):\", mean_squared_error(y_test, model.predict(X_test_scaled)))\n",
    "\n",
    "# ========== Define Optimization Space ==========\n",
    "param_bounds = {\n",
    "    'Fe': (60, 100), 'Ni': (0, 20), 'C': (0, 3), 'Mn': (0, 30), 'Cr': (0, 20),\n",
    "    'Mo': (0, 5), 'Si': (0, 2),\n",
    "    'atomic concentration': (8e22, 2e24),\n",
    "    'electron affinity': (50, 140),\n",
    "    'ionic radii': (0.68, 0.81),\n",
    "    'work function': (4.3, 5),\n",
    "    'first ionization': (1200, 6000),\n",
    "    'valence electron': (4.4, 5),\n",
    "    'enthalpy of vacancies': (130, 180)\n",
    "}\n",
    "\n",
    "dimensions = [Real(*param_bounds[k]) for k in opt_params]\n",
    "\n",
    "# ========== Define Objective Function ==========\n",
    "def objective(params):\n",
    "    df_params = pd.DataFrame([params], columns=opt_params)\n",
    "    df_scaled = scaler.transform(df_params)\n",
    "    pred = model.predict(df_scaled)[0]\n",
    "    return -pred  # We want to maximize ys\n",
    "\n",
    "# ========== Bayesian Optimization Loop ==========\n",
    "best_results = []\n",
    "\n",
    "for run_id in range(3):  # Try 3 different seeds\n",
    "    optimizer = Optimizer(dimensions=dimensions, base_estimator='GP', n_initial_points=10, random_state=42 + run_id)\n",
    "\n",
    "    for step in range(50):  \n",
    "        next_point = optimizer.ask()\n",
    "        loss = objective(next_point)\n",
    "        optimizer.tell(next_point, loss)\n",
    "\n",
    "    best = optimizer.get_result().x\n",
    "    best_dict = dict(zip(opt_params, best))\n",
    "    best_scaled = scaler.transform(pd.DataFrame([best_dict]))\n",
    "    best_pred = model.predict(best_scaled)[0]\n",
    "\n",
    "    best_results.append((best_dict, best_pred))\n",
    "\n",
    "# ========== Display Results ==========\n",
    "for i, (params, pred) in enumerate(best_results, 1):\n",
    "    print(f\"\\nRun {i}:\")\n",
    "    print(\"Best Parameters:\")\n",
    "    for k, v in params.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    print(f\"Predicted ys: {pred:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36a74ea-c951-4ebb-baca-466a474496bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
